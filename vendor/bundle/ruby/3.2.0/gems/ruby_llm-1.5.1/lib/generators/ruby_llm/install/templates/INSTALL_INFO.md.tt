# RubyLLM Rails Setup Complete!

Thanks for installing RubyLLM in your Rails application. Here's what was created:

## Models

- `<%= options[:chat_model_name] %>` - Stores chat sessions and their associated model ID
- `<%= options[:message_model_name] %>` - Stores individual messages in a chat
- `<%= options[:tool_call_model_name] %>` - Stores tool calls made by language models

**Note:** Do not add `validates :content, presence: true` to your Message model - RubyLLM creates empty assistant messages before API calls for streaming support.

## Configuration Options

The generator supports the following options to customize model names:

```bash
rails generate ruby_llm:install \
  --chat-model-name=Conversation \
  --message-model-name=ChatMessage \
  --tool-call-model-name=FunctionCall
```

This is useful when you need to avoid namespace collisions with existing models in your application. Table names will be automatically derived from the model names following Rails conventions.

## Next Steps

1. **Run migrations:**
   ```bash
   rails db:migrate
   ```

   **Database Note:** The migrations use `jsonb` for PostgreSQL and `json` for MySQL/SQLite automatically.

2. **Set your API keys** in `config/initializers/ruby_llm.rb` or using environment variables:
   ```ruby
   # config/initializers/ruby_llm.rb
   RubyLLM.configure do |config|
     config.openai_api_key = ENV['OPENAI_API_KEY']
     config.anthropic_api_key = ENV['ANTHROPIC_API_KEY']
     config.gemini_api_key = ENV['GEMINI_API_KEY']
     # ... add other providers as needed
   end
   ```

3. **Start using RubyLLM in your code:**
   ```ruby
   # Basic usage
   chat = <%= options[:chat_model_name] %>.create!(model_id: 'gpt-4.1-nano')
   response = chat.ask("What is Ruby on Rails?")

   # With file attachments (requires ActiveStorage setup)
   chat.ask("What's in this file?", with: "report.pdf")
   chat.ask("Analyze these files", with: ["image.jpg", "data.csv", "notes.txt"])
   ```

4. **For streaming responses** with Hotwire/Turbo:
   ```ruby
   # app/models/<%= options[:message_model_name].underscore %>.rb
   class <%= options[:message_model_name] %> < ApplicationRecord
     acts_as_message

     # Helper to broadcast chunks during streaming
     def broadcast_append_chunk(chunk_content)
       broadcast_append_to [ chat, "messages" ],
         target: dom_id(self, "content"),
         html: chunk_content
     end
   end

   # app/jobs/chat_stream_job.rb
   class ChatStreamJob < ApplicationJob
     def perform(chat_id, user_content)
       chat = <%= options[:chat_model_name] %>.find(chat_id)
       chat.ask(user_content) do |chunk|
         assistant_message = chat.messages.last
         if chunk.content && assistant_message
           assistant_message.broadcast_append_chunk(chunk.content)
         end
       end
     end
   end

   # In your controller
   ChatStreamJob.perform_later(@chat.id, params[:content])
   ```

## Optional: ActiveStorage for Attachments

If you want to use file attachments (PDFs, images, etc.), set up ActiveStorage:

```bash
rails active_storage:install
rails db:migrate
```

Then add to your Message model:
```ruby
class <%= options[:message_model_name] %> < ApplicationRecord
  acts_as_message
  has_many_attached :attachments
end
```

## Learn More

- See the [Rails Integration Guide](https://rubyllm.com/guides/rails) for detailed examples
- Visit the [RubyLLM Documentation](https://rubyllm.com) for full API reference